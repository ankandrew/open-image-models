{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Image Models","text":"<p>Ready-to-use models for a range of computer vision tasks like detection, classification, and more. With ONNX support, you get fast and accurate results right out of the box.</p> <p>Easily integrate these models into your apps for real-time processing\u2014ideal for edge devices, cloud setups, or production environments. In one line of code, you can have powerful model inference running!</p> <pre><code>from open_image_models import LicensePlateDetector\n\nlp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-256-license-plate-end2end\")\nlp_detector.predict(\"path/to/license_plate_image.jpg\")\n</code></pre> Info <p>\u2728 That\u2019s it! Fast, accurate computer vision models with just one line of code.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\ude80 Pre-trained: Models are ready for immediate use, no additional training required.</li> <li>\ud83c\udf1f ONNX: Cross-platform support for fast inference on both CPU and GPU environments.</li> <li>\u26a1 Performance: Optimized for both speed and accuracy, ensuring efficient real-time applications.</li> <li>\ud83d\udcbb Simple API: Power up your applications with robust model inference in just one line of code.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to the repo are greatly appreciated. Whether it's bug fixes, feature enhancements, or new models, your contributions are warmly welcomed.</p> <p>To start contributing or to begin development, you can follow these steps:</p> <ol> <li>Clone repo     <pre><code>git clone https://github.com/ankandrew/open-image-models.git\n</code></pre></li> <li>Install all dependencies using Poetry:     <pre><code>make install\n</code></pre></li> <li>To ensure your changes pass linting and tests before submitting a PR:     <pre><code>make checks\n</code></pre></li> </ol>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<pre><code>pip install open-image-models\n</code></pre> Info <p>The models currently run using the ONNX framework. Depending on your system, you may want to install support for GPU or other hardware accelerators. Use optional extras like <code>gpu</code>, <code>openvino</code>, <code>directml</code>, or <code>qnn</code> to tailor the installation to your machine. For example: <code>pip install open-image-models[gpu]</code>.</p>"},{"location":"reference/","title":"\ud83d\udee0 Pipelines Overview","text":""},{"location":"reference/#license-plate-detection","title":"License Plate Detection","text":"<p>\ud83d\ude97 License Plate Detection allows you to detect and identify license plates in images using a specialized pipeline based on the YOLOv9 model.</p> <p>The <code>LicensePlateDetector</code> is specialized for license plate detection. It utilizes the YOLOv9 object detection model to recognize license plates in images.</p> <p>               Bases: <code>YoloV9ObjectDetector</code></p> <p>Specialized detector for license plates using YoloV9 model. Inherits from YoloV9ObjectDetector and sets up license plate specific configuration.</p> <p>Parameters:</p> Name Type Description Default <code>detection_model</code> <code>PlateDetectorModel</code> <p>Detection model to use, see <code>PlateDetectorModel</code>.</p> required <code>conf_thresh</code> <code>float</code> <p>Confidence threshold for filtering predictions.</p> <code>0.25</code> <code>providers</code> <code>Sequence[str | tuple[str, dict]] | None</code> <p>Optional sequence of providers in order of decreasing precedence. If not specified, all available providers are used.</p> <code>None</code> <code>sess_options</code> <code>SessionOptions</code> <p>Advanced session options for ONNX Runtime.</p> <code>None</code> Source code in <code>open_image_models/detection/pipeline/license_plate.py</code> <pre><code>def __init__(\n    self,\n    detection_model: PlateDetectorModel,\n    conf_thresh: float = 0.25,\n    providers: Sequence[str | tuple[str, dict]] | None = None,\n    sess_options: ort.SessionOptions = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the LicensePlateDetector with the specified detection model and inference device.\n\n    Args:\n        detection_model: Detection model to use, see `PlateDetectorModel`.\n        conf_thresh: Confidence threshold for filtering predictions.\n        providers: Optional sequence of providers in order of decreasing precedence. If not specified, all available\n            providers are used.\n        sess_options: Advanced session options for ONNX Runtime.\n    \"\"\"\n    # Download model if needed\n    detector_model_path = download_model(detection_model)\n    super().__init__(\n        model_path=detector_model_path,\n        conf_thresh=conf_thresh,\n        class_labels=[\"License Plate\"],\n        providers=providers,\n        sess_options=sess_options,\n    )\n    LOGGER.info(\"Initialized LicensePlateDetector with model %s\", detector_model_path)\n</code></pre>"},{"location":"reference/#open_image_models.detection.pipeline.license_plate.LicensePlateDetector.predict","title":"predict","text":"<pre><code>predict(images: ndarray) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(\n    images: list[ndarray],\n) -&gt; list[list[DetectionResult]]\n</code></pre><pre><code>predict(images: str) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(images: list[str]) -&gt; list[list[DetectionResult]]\n</code></pre><pre><code>predict(images: PathLike[str]) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(\n    images: list[PathLike[str]],\n) -&gt; list[list[DetectionResult]]\n</code></pre> <pre><code>predict(\n    images: Any,\n) -&gt; list[DetectionResult] | list[list[DetectionResult]]\n</code></pre> <p>Perform license plate detection on one or multiple images.</p> <p>This method is a specialized version of the <code>YoloV9ObjectDetector.predict</code> method, focusing on detecting license plates in images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Any</code> <p>A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,     or a list of image file paths.</p> required <p>Returns:</p> Type Description <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>A list of <code>DetectionResult</code> for a single image input, or a list of lists of <code>DetectionResult</code> for multiple images.</p> <p>Example usage:</p> <pre><code>from open_image_models import LicensePlateDetector\n\nlp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-384-license-plate-end2end\")\nlp_detector.predict(\"path/to/license_plate_image.jpg\")\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the image could not be loaded or processed.</p> Source code in <code>open_image_models/detection/pipeline/license_plate.py</code> <pre><code>def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n    \"\"\"\n    Perform license plate detection on one or multiple images.\n\n    This method is a specialized version of the `YoloV9ObjectDetector.predict` method,\n    focusing on detecting license plates in images.\n\n    Args:\n        images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                or a list of image file paths.\n\n    Returns:\n        A list of `DetectionResult` for a single image input, or a list of lists of `DetectionResult` for multiple\n            images.\n\n    Example usage:\n\n    ```python\n    from open_image_models import LicensePlateDetector\n\n    lp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-384-license-plate-end2end\")\n    lp_detector.predict(\"path/to/license_plate_image.jpg\")\n    ```\n\n    Raises:\n        ValueError: If the image could not be loaded or processed.\n    \"\"\"\n    return super().predict(images)\n</code></pre>"},{"location":"reference/#core-api-documentation","title":"Core API Documentation","text":"<p>The <code>core</code> module provides base classes and protocols for object detection models, including essential data structures like <code>BoundingBox</code> and <code>DetectionResult</code>.</p>"},{"location":"reference/#core-components","title":"\ud83d\udd27 Core Components","text":"<p>The following components are used across detection pipelines and models:</p> <ul> <li><code>BoundingBox</code>: Represents a bounding box for detected objects.</li> <li><code>DetectionResult</code>: Stores label, confidence, and bounding box for a detection.</li> <li><code>ObjectDetector</code>: Protocol defining essential methods like <code>predict</code>, <code>show_benchmark</code>, and <code>display_predictions</code>.</li> </ul> <p>Open Image Models HUB.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox","title":"BoundingBox  <code>dataclass</code>","text":"<pre><code>BoundingBox(x1: int, y1: int, x2: int, y2: int)\n</code></pre> <p>Represents a bounding box with top-left and bottom-right coordinates.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.x1","title":"x1  <code>instance-attribute</code>","text":"<pre><code>x1: int\n</code></pre> <p>X-coordinate of the top-left corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.y1","title":"y1  <code>instance-attribute</code>","text":"<pre><code>y1: int\n</code></pre> <p>Y-coordinate of the top-left corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.x2","title":"x2  <code>instance-attribute</code>","text":"<pre><code>x2: int\n</code></pre> <p>X-coordinate of the bottom-right corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.y2","title":"y2  <code>instance-attribute</code>","text":"<pre><code>y2: int\n</code></pre> <p>Y-coordinate of the bottom-right corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: int\n</code></pre> <p>Returns the width of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: int\n</code></pre> <p>Returns the height of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: int\n</code></pre> <p>Returns the area of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: tuple[float, float]\n</code></pre> <p>Returns the (x, y) coordinates of the center of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.intersection","title":"intersection","text":"<pre><code>intersection(other: BoundingBox) -&gt; Optional[BoundingBox]\n</code></pre> <p>Returns the intersection of this bounding box with another bounding box. If they do not intersect, returns None.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def intersection(self, other: \"BoundingBox\") -&gt; Optional[\"BoundingBox\"]:\n    \"\"\"\n    Returns the intersection of this bounding box with another bounding box. If they do not intersect, returns None.\n    \"\"\"\n    x1 = max(self.x1, other.x1)\n    y1 = max(self.y1, other.y1)\n    x2 = min(self.x2, other.x2)\n    y2 = min(self.y2, other.y2)\n\n    if x2 &gt; x1 and y2 &gt; y1:\n        return BoundingBox(x1, y1, x2, y2)\n\n    return None\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.iou","title":"iou","text":"<pre><code>iou(other: BoundingBox) -&gt; float\n</code></pre> <p>Computes the Intersection-over-Union (IoU) between this bounding box and another bounding box.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def iou(self, other: \"BoundingBox\") -&gt; float:\n    \"\"\"\n    Computes the Intersection-over-Union (IoU) between this bounding box and another bounding box.\n    \"\"\"\n    inter = self.intersection(other)\n\n    if inter is None:\n        return 0.0\n\n    inter_area = inter.area\n    union_area = self.area + other.area - inter_area\n    return inter_area / union_area if union_area &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; tuple[int, int, int, int]\n</code></pre> <p>Converts bounding box to (x, y, width, height) format, where (x, y) is the top-left corner.</p> <p>:returns: A tuple containing the top-left x and y coordinates, width, and height of the bounding box.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def to_xywh(self) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    Converts bounding box to (x, y, width, height) format, where (x, y) is the top-left corner.\n\n    :returns: A tuple containing the top-left x and y coordinates, width, and height of the bounding box.\n    \"\"\"\n    return self.x1, self.y1, self.width, self.height\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.clamp","title":"clamp","text":"<pre><code>clamp(max_width: int, max_height: int) -&gt; BoundingBox\n</code></pre> <p>Returns a new <code>BoundingBox</code> with coordinates clamped within the range [0, max_width] and [0, max_height].</p> <p>:param max_width: The maximum width. :param max_height: The maximum height. :return: A new, clamped <code>BoundingBox</code>.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def clamp(self, max_width: int, max_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Returns a new `BoundingBox` with coordinates clamped within the range [0, max_width] and [0, max_height].\n\n    :param max_width: The maximum width.\n    :param max_height: The maximum height.\n    :return: A new, clamped `BoundingBox`.\n    \"\"\"\n    return BoundingBox(\n        x1=max(0, min(self.x1, max_width)),\n        y1=max(0, min(self.y1, max_height)),\n        x2=max(0, min(self.x2, max_width)),\n        y2=max(0, min(self.y2, max_height)),\n    )\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.is_valid","title":"is_valid","text":"<pre><code>is_valid(frame_width: int, frame_height: int) -&gt; bool\n</code></pre> <p>Checks if the bounding box is valid by ensuring that:</p> <ol> <li>The coordinates are in the correct order (x1 &lt; x2 and y1 &lt; y2).</li> <li>The bounding box lies entirely within the frame boundaries.</li> </ol> <p>:param frame_width: The width of the frame. :param frame_height: The height of the frame. :return: True if the bounding box is valid, False otherwise.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def is_valid(self, frame_width: int, frame_height: int) -&gt; bool:\n    \"\"\"\n    Checks if the bounding box is valid by ensuring that:\n\n    1. The coordinates are in the correct order (x1 &lt; x2 and y1 &lt; y2).\n    2. The bounding box lies entirely within the frame boundaries.\n\n    :param frame_width: The width of the frame.\n    :param frame_height: The height of the frame.\n    :return: True if the bounding box is valid, False otherwise.\n    \"\"\"\n    return 0 &lt;= self.x1 &lt; self.x2 &lt;= frame_width and 0 &lt;= self.y1 &lt; self.y2 &lt;= frame_height\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult","title":"DetectionResult  <code>dataclass</code>","text":"<pre><code>DetectionResult(\n    label: str, confidence: float, bounding_box: BoundingBox\n)\n</code></pre> <p>Represents the result of an object detection.</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.label","title":"label  <code>instance-attribute</code>","text":"<pre><code>label: str\n</code></pre> <p>Detected object label</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.confidence","title":"confidence  <code>instance-attribute</code>","text":"<pre><code>confidence: float\n</code></pre> <p>Confidence score of the detection</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.bounding_box","title":"bounding_box  <code>instance-attribute</code>","text":"<pre><code>bounding_box: BoundingBox\n</code></pre> <p>Bounding box of the detected object</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.from_detection_data","title":"from_detection_data  <code>classmethod</code>","text":"<pre><code>from_detection_data(\n    bbox_data: tuple[int, int, int, int],\n    confidence: float,\n    class_id: str,\n) -&gt; DetectionResult\n</code></pre> <p>Creates a <code>DetectionResult</code> instance from bounding box data, confidence, and a class label.</p> <p>:param bbox_data: A tuple containing bounding box coordinates (x1, y1, x2, y2). :param confidence: The detection confidence score. :param class_id: The detected class label as a string. :return: A <code>DetectionResult</code> instance.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>@classmethod\ndef from_detection_data(\n    cls,\n    bbox_data: tuple[int, int, int, int],\n    confidence: float,\n    class_id: str,\n) -&gt; \"DetectionResult\":\n    \"\"\"\n    Creates a `DetectionResult` instance from bounding box data, confidence, and a class label.\n\n    :param bbox_data: A tuple containing bounding box coordinates (x1, y1, x2, y2).\n    :param confidence: The detection confidence score.\n    :param class_id: The detected class label as a string.\n    :return: A `DetectionResult` instance.\n    \"\"\"\n    bounding_box = BoundingBox(*bbox_data)\n    return cls(class_id, confidence, bounding_box)\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector","title":"ObjectDetector","text":"<p>               Bases: <code>Protocol</code></p>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.predict","title":"predict","text":"<pre><code>predict(\n    images: Any,\n) -&gt; list[DetectionResult] | list[list[DetectionResult]]\n</code></pre> <p>Perform object detection on one or multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Any</code> <p>A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,     or a list of image file paths.</p> required <p>Returns:</p> Type Description <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>A list of DetectionResult for a single image input,</p> <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>or a list of lists of DetectionResult for multiple images.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n    \"\"\"\n    Perform object detection on one or multiple images.\n\n    Args:\n        images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                or a list of image file paths.\n\n    Returns:\n        A list of DetectionResult for a single image input,\n        or a list of lists of DetectionResult for multiple images.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.show_benchmark","title":"show_benchmark","text":"<pre><code>show_benchmark(num_runs: int = 10) -&gt; None\n</code></pre> <p>Display the benchmark results of the model with a single random image.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>Number of times to run inference on the image for averaging.</p> <code>10</code> Displays <p>Model information and benchmark results in a formatted table.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def show_benchmark(self, num_runs: int = 10) -&gt; None:\n    \"\"\"\n    Display the benchmark results of the model with a single random image.\n\n    Args:\n        num_runs: Number of times to run inference on the image for averaging.\n\n    Displays:\n        Model information and benchmark results in a formatted table.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.display_predictions","title":"display_predictions","text":"<pre><code>display_predictions(image: ndarray) -&gt; ndarray\n</code></pre> <p>Run object detection on the input image and display the predictions on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>An input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The image with bounding boxes and labels drawn on it.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def display_predictions(self, image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Run object detection on the input image and display the predictions on the image.\n\n    Args:\n        image: An input image as a numpy array.\n\n    Returns:\n        The image with bounding boxes and labels drawn on it.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.hub.PlateDetectorModel","title":"PlateDetectorModel  <code>module-attribute</code>","text":"<pre><code>PlateDetectorModel = Literal[\n    \"yolo-v9-s-608-license-plate-end2end\",\n    \"yolo-v9-t-640-license-plate-end2end\",\n    \"yolo-v9-t-512-license-plate-end2end\",\n    \"yolo-v9-t-416-license-plate-end2end\",\n    \"yolo-v9-t-384-license-plate-end2end\",\n    \"yolo-v9-t-256-license-plate-end2end\",\n]\n</code></pre> <p>Available ONNX models for doing detection.</p>"}]}