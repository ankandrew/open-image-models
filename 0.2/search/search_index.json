{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Open Image Models","text":"<p>Ready-to-use models for a range of computer vision tasks like detection, classification, and more. With ONNX support, you get fast and accurate results right out of the box.</p> <p>Easily integrate these models into your apps for real-time processing\u2014ideal for edge devices, cloud setups, or production environments. In one line of code, you can have powerful model inference running!</p> <pre><code>from open_image_models import LicensePlateDetector\n\nlp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-256-license-plate-end2end\")\nlp_detector.predict(\"path/to/license_plate_image.jpg\")\n</code></pre> Info <p>\u2728 That\u2019s it! Fast, accurate computer vision models with just one line of code.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>\ud83d\ude80 Pre-trained: Models are ready for immediate use, no additional training required.</li> <li>\ud83c\udf1f ONNX: Cross-platform support for fast inference on both CPU and GPU environments.</li> <li>\u26a1 Performance: Optimized for both speed and accuracy, ensuring efficient real-time applications.</li> <li>\ud83d\udcbb Simple API: Power up your applications with robust model inference in just one line of code.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to the repo are greatly appreciated. Whether it's bug fixes, feature enhancements, or new models, your contributions are warmly welcomed.</p> <p>To start contributing or to begin development, you can follow these steps:</p> <ol> <li>Clone repo     <pre><code>git clone https://github.com/ankandrew/open-image-models.git\n</code></pre></li> <li>Install all dependencies using Poetry:     <pre><code>poetry install --all-extras\n</code></pre></li> <li>To ensure your changes pass linting and tests before submitting a PR:     <pre><code>make checks\n</code></pre></li> </ol>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<pre><code>pip install open-image-models\n</code></pre> Info <p>Currently, ONNX framework is used to run the models. When installing <code>open-image-models</code>, the corresponding version GPU/CPU should be installed based on your machine architecture.</p>"},{"location":"reference/","title":"\ud83d\udee0 Pipelines Overview","text":""},{"location":"reference/#license-plate-detection","title":"License Plate Detection","text":"<p>\ud83d\ude97 License Plate Detection allows you to detect and identify license plates in images using a specialized pipeline based on the YOLOv9 model.</p> <p>The <code>LicensePlateDetector</code> is specialized for license plate detection. It utilizes the YOLOv9 object detection model to recognize license plates in images.</p> <p>               Bases: <code>YoloV9ObjectDetector</code></p> <p>Specialized detector for license plates using YoloV9 model. Inherits from YoloV9ObjectDetector and sets up license plate specific configuration.</p> Source code in <code>open_image_models/detection/pipeline/license_plate.py</code> <pre><code>class LicensePlateDetector(YoloV9ObjectDetector):\n    \"\"\"\n    Specialized detector for license plates using YoloV9 model.\n    Inherits from YoloV9ObjectDetector and sets up license plate specific configuration.\n    \"\"\"\n\n    def __init__(\n        self,\n        detection_model: PlateDetectorModel,\n        conf_thresh: float = 0.25,\n        providers: Sequence[str | tuple[str, dict]] | None = None,\n        sess_options: ort.SessionOptions = None,\n    ) -&gt; None:\n        \"\"\"\n        Initializes the LicensePlateDetector with the specified detection model and inference device.\n\n        Args:\n            detection_model: Detection model to use, see `PlateDetectorModel`.\n            conf_thresh: Confidence threshold for filtering predictions.\n            providers: Optional sequence of providers in order of decreasing precedence. If not specified, all available\n                providers are used.\n            sess_options: Advanced session options for ONNX Runtime.\n        \"\"\"\n        # Download model if needed\n        detector_model_path = download_model(detection_model)\n        super().__init__(\n            model_path=detector_model_path,\n            conf_thresh=conf_thresh,\n            class_labels=[\"License Plate\"],\n            providers=providers,\n            sess_options=sess_options,\n        )\n        LOGGER.info(\"Initialized LicensePlateDetector with model %s\", detector_model_path)\n\n    # pylint: disable=duplicate-code\n    @overload\n    def predict(self, images: np.ndarray) -&gt; list[DetectionResult]: ...\n\n    @overload\n    def predict(self, images: list[np.ndarray]) -&gt; list[list[DetectionResult]]: ...\n\n    @overload\n    def predict(self, images: str) -&gt; list[DetectionResult]: ...\n\n    @overload\n    def predict(self, images: list[str]) -&gt; list[list[DetectionResult]]: ...\n\n    @overload\n    def predict(self, images: os.PathLike[str]) -&gt; list[DetectionResult]: ...\n\n    @overload\n    def predict(self, images: list[os.PathLike[str]]) -&gt; list[list[DetectionResult]]: ...\n\n    def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n        \"\"\"\n        Perform license plate detection on one or multiple images.\n\n        This method is a specialized version of the `YoloV9ObjectDetector.predict` method,\n        focusing on detecting license plates in images.\n\n        Args:\n            images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                    or a list of image file paths.\n\n        Returns:\n            A list of `DetectionResult` for a single image input, or a list of lists of `DetectionResult` for multiple\n                images.\n\n        Example usage:\n\n        ```python\n        from open_image_models import LicensePlateDetector\n\n        lp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-384-license-plate-end2end\")\n        lp_detector.predict(\"path/to/license_plate_image.jpg\")\n        ```\n\n        Raises:\n            ValueError: If the image could not be loaded or processed.\n        \"\"\"\n        return super().predict(images)\n</code></pre>"},{"location":"reference/#open_image_models.detection.pipeline.license_plate.LicensePlateDetector.__init__","title":"<code>__init__(detection_model, conf_thresh=0.25, providers=None, sess_options=None)</code>","text":"<p>Initializes the LicensePlateDetector with the specified detection model and inference device.</p> <p>Parameters:</p> Name Type Description Default <code>detection_model</code> <code>PlateDetectorModel</code> <p>Detection model to use, see <code>PlateDetectorModel</code>.</p> required <code>conf_thresh</code> <code>float</code> <p>Confidence threshold for filtering predictions.</p> <code>0.25</code> <code>providers</code> <code>Sequence[str | tuple[str, dict]] | None</code> <p>Optional sequence of providers in order of decreasing precedence. If not specified, all available providers are used.</p> <code>None</code> <code>sess_options</code> <code>SessionOptions</code> <p>Advanced session options for ONNX Runtime.</p> <code>None</code> Source code in <code>open_image_models/detection/pipeline/license_plate.py</code> <pre><code>def __init__(\n    self,\n    detection_model: PlateDetectorModel,\n    conf_thresh: float = 0.25,\n    providers: Sequence[str | tuple[str, dict]] | None = None,\n    sess_options: ort.SessionOptions = None,\n) -&gt; None:\n    \"\"\"\n    Initializes the LicensePlateDetector with the specified detection model and inference device.\n\n    Args:\n        detection_model: Detection model to use, see `PlateDetectorModel`.\n        conf_thresh: Confidence threshold for filtering predictions.\n        providers: Optional sequence of providers in order of decreasing precedence. If not specified, all available\n            providers are used.\n        sess_options: Advanced session options for ONNX Runtime.\n    \"\"\"\n    # Download model if needed\n    detector_model_path = download_model(detection_model)\n    super().__init__(\n        model_path=detector_model_path,\n        conf_thresh=conf_thresh,\n        class_labels=[\"License Plate\"],\n        providers=providers,\n        sess_options=sess_options,\n    )\n    LOGGER.info(\"Initialized LicensePlateDetector with model %s\", detector_model_path)\n</code></pre>"},{"location":"reference/#open_image_models.detection.pipeline.license_plate.LicensePlateDetector.predict","title":"<code>predict(images)</code>","text":"<pre><code>predict(images: np.ndarray) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(\n    images: list[np.ndarray],\n) -&gt; list[list[DetectionResult]]\n</code></pre><pre><code>predict(images: str) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(images: list[str]) -&gt; list[list[DetectionResult]]\n</code></pre><pre><code>predict(images: os.PathLike[str]) -&gt; list[DetectionResult]\n</code></pre><pre><code>predict(\n    images: list[os.PathLike[str]],\n) -&gt; list[list[DetectionResult]]\n</code></pre> <p>Perform license plate detection on one or multiple images.</p> <p>This method is a specialized version of the <code>YoloV9ObjectDetector.predict</code> method, focusing on detecting license plates in images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Any</code> <p>A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,     or a list of image file paths.</p> required <p>Returns:</p> Type Description <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>A list of <code>DetectionResult</code> for a single image input, or a list of lists of <code>DetectionResult</code> for multiple images.</p> <p>Example usage:</p> <pre><code>from open_image_models import LicensePlateDetector\n\nlp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-384-license-plate-end2end\")\nlp_detector.predict(\"path/to/license_plate_image.jpg\")\n</code></pre> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the image could not be loaded or processed.</p> Source code in <code>open_image_models/detection/pipeline/license_plate.py</code> <pre><code>def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n    \"\"\"\n    Perform license plate detection on one or multiple images.\n\n    This method is a specialized version of the `YoloV9ObjectDetector.predict` method,\n    focusing on detecting license plates in images.\n\n    Args:\n        images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                or a list of image file paths.\n\n    Returns:\n        A list of `DetectionResult` for a single image input, or a list of lists of `DetectionResult` for multiple\n            images.\n\n    Example usage:\n\n    ```python\n    from open_image_models import LicensePlateDetector\n\n    lp_detector = LicensePlateDetector(detection_model=\"yolo-v9-t-384-license-plate-end2end\")\n    lp_detector.predict(\"path/to/license_plate_image.jpg\")\n    ```\n\n    Raises:\n        ValueError: If the image could not be loaded or processed.\n    \"\"\"\n    return super().predict(images)\n</code></pre>"},{"location":"reference/#core-api-documentation","title":"Core API Documentation","text":"<p>The <code>core</code> module provides base classes and protocols for object detection models, including essential data structures like <code>BoundingBox</code> and <code>DetectionResult</code>.</p>"},{"location":"reference/#core-components","title":"\ud83d\udd27 Core Components","text":"<p>The following components are used across detection pipelines and models:</p> <ul> <li><code>BoundingBox</code>: Represents a bounding box for detected objects.</li> <li><code>DetectionResult</code>: Stores label, confidence, and bounding box for a detection.</li> <li><code>ObjectDetector</code>: Protocol defining essential methods like <code>predict</code>, <code>show_benchmark</code>, and <code>display_predictions</code>.</li> </ul> <p>Open Image Models HUB.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox","title":"<code>BoundingBox</code>  <code>dataclass</code>","text":"<p>Represents a bounding box with top-left and bottom-right coordinates.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>@dataclass(frozen=True)\nclass BoundingBox:\n    \"\"\"\n    Represents a bounding box with top-left and bottom-right coordinates.\n    \"\"\"\n\n    x1: int\n    \"\"\"X-coordinate of the top-left corner\"\"\"\n    y1: int\n    \"\"\"Y-coordinate of the top-left corner\"\"\"\n    x2: int\n    \"\"\"X-coordinate of the bottom-right corner\"\"\"\n    y2: int\n    \"\"\"Y-coordinate of the bottom-right corner\"\"\"\n\n    @property\n    def width(self) -&gt; int:\n        \"\"\"Returns the width of the bounding box.\"\"\"\n        return self.x2 - self.x1\n\n    @property\n    def height(self) -&gt; int:\n        \"\"\"Returns the height of the bounding box.\"\"\"\n        return self.y2 - self.y1\n\n    @property\n    def area(self) -&gt; int:\n        \"\"\"Returns the area of the bounding box.\"\"\"\n        return self.width * self.height\n\n    @property\n    def center(self) -&gt; tuple[float, float]:\n        \"\"\"\n        Returns the (x, y) coordinates of the center of the bounding box.\n        \"\"\"\n        cx = (self.x1 + self.x2) / 2.0\n        cy = (self.y1 + self.y2) / 2.0\n\n        return cx, cy\n\n    def intersection(self, other: \"BoundingBox\") -&gt; Optional[\"BoundingBox\"]:\n        \"\"\"\n        Returns the intersection of this bounding box with another bounding box. If they do not intersect, returns None.\n        \"\"\"\n        x1 = max(self.x1, other.x1)\n        y1 = max(self.y1, other.y1)\n        x2 = min(self.x2, other.x2)\n        y2 = min(self.y2, other.y2)\n\n        if x2 &gt; x1 and y2 &gt; y1:\n            return BoundingBox(x1, y1, x2, y2)\n\n        return None\n\n    def iou(self, other: \"BoundingBox\") -&gt; float:\n        \"\"\"\n        Computes the Intersection-over-Union (IoU) between this bounding box and another bounding box.\n        \"\"\"\n        inter = self.intersection(other)\n\n        if inter is None:\n            return 0.0\n\n        inter_area = inter.area\n        union_area = self.area + other.area - inter_area\n        return inter_area / union_area if union_area &gt; 0 else 0.0\n\n    def to_xywh(self) -&gt; tuple[int, int, int, int]:\n        \"\"\"\n        Converts bounding box to (x, y, width, height) format, where (x, y) is the top-left corner.\n        \"\"\"\n        return self.x1, self.y1, self.width, self.height\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.area","title":"<code>area</code>  <code>property</code>","text":"<p>Returns the area of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.center","title":"<code>center</code>  <code>property</code>","text":"<p>Returns the (x, y) coordinates of the center of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.height","title":"<code>height</code>  <code>property</code>","text":"<p>Returns the height of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.width","title":"<code>width</code>  <code>property</code>","text":"<p>Returns the width of the bounding box.</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.x1","title":"<code>x1</code>  <code>instance-attribute</code>","text":"<p>X-coordinate of the top-left corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.x2","title":"<code>x2</code>  <code>instance-attribute</code>","text":"<p>X-coordinate of the bottom-right corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.y1","title":"<code>y1</code>  <code>instance-attribute</code>","text":"<p>Y-coordinate of the top-left corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.y2","title":"<code>y2</code>  <code>instance-attribute</code>","text":"<p>Y-coordinate of the bottom-right corner</p>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.intersection","title":"<code>intersection(other)</code>","text":"<p>Returns the intersection of this bounding box with another bounding box. If they do not intersect, returns None.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def intersection(self, other: \"BoundingBox\") -&gt; Optional[\"BoundingBox\"]:\n    \"\"\"\n    Returns the intersection of this bounding box with another bounding box. If they do not intersect, returns None.\n    \"\"\"\n    x1 = max(self.x1, other.x1)\n    y1 = max(self.y1, other.y1)\n    x2 = min(self.x2, other.x2)\n    y2 = min(self.y2, other.y2)\n\n    if x2 &gt; x1 and y2 &gt; y1:\n        return BoundingBox(x1, y1, x2, y2)\n\n    return None\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.iou","title":"<code>iou(other)</code>","text":"<p>Computes the Intersection-over-Union (IoU) between this bounding box and another bounding box.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def iou(self, other: \"BoundingBox\") -&gt; float:\n    \"\"\"\n    Computes the Intersection-over-Union (IoU) between this bounding box and another bounding box.\n    \"\"\"\n    inter = self.intersection(other)\n\n    if inter is None:\n        return 0.0\n\n    inter_area = inter.area\n    union_area = self.area + other.area - inter_area\n    return inter_area / union_area if union_area &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.BoundingBox.to_xywh","title":"<code>to_xywh()</code>","text":"<p>Converts bounding box to (x, y, width, height) format, where (x, y) is the top-left corner.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def to_xywh(self) -&gt; tuple[int, int, int, int]:\n    \"\"\"\n    Converts bounding box to (x, y, width, height) format, where (x, y) is the top-left corner.\n    \"\"\"\n    return self.x1, self.y1, self.width, self.height\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult","title":"<code>DetectionResult</code>  <code>dataclass</code>","text":"<p>Represents the result of an object detection.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>@dataclass(frozen=True)\nclass DetectionResult:\n    \"\"\"\n    Represents the result of an object detection.\n    \"\"\"\n\n    label: str\n    \"\"\"Detected object label\"\"\"\n    confidence: float\n    \"\"\"Confidence score of the detection\"\"\"\n    bounding_box: BoundingBox\n    \"\"\"Bounding box of the detected object\"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.bounding_box","title":"<code>bounding_box</code>  <code>instance-attribute</code>","text":"<p>Bounding box of the detected object</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.confidence","title":"<code>confidence</code>  <code>instance-attribute</code>","text":"<p>Confidence score of the detection</p>"},{"location":"reference/#open_image_models.detection.core.base.DetectionResult.label","title":"<code>label</code>  <code>instance-attribute</code>","text":"<p>Detected object label</p>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector","title":"<code>ObjectDetector</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>class ObjectDetector(Protocol):\n    def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n        \"\"\"\n        Perform object detection on one or multiple images.\n\n        Args:\n            images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                    or a list of image file paths.\n\n        Returns:\n            A list of DetectionResult for a single image input,\n            or a list of lists of DetectionResult for multiple images.\n        \"\"\"\n\n    def show_benchmark(self, num_runs: int = 10) -&gt; None:\n        \"\"\"\n        Display the benchmark results of the model with a single random image.\n\n        Args:\n            num_runs: Number of times to run inference on the image for averaging.\n\n        Displays:\n            Model information and benchmark results in a formatted table.\n        \"\"\"\n\n    def display_predictions(self, image: np.ndarray) -&gt; np.ndarray:\n        \"\"\"\n        Run object detection on the input image and display the predictions on the image.\n\n        Args:\n            image: An input image as a numpy array.\n\n        Returns:\n            The image with bounding boxes and labels drawn on it.\n        \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.display_predictions","title":"<code>display_predictions(image)</code>","text":"<p>Run object detection on the input image and display the predictions on the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>An input image as a numpy array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The image with bounding boxes and labels drawn on it.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def display_predictions(self, image: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    Run object detection on the input image and display the predictions on the image.\n\n    Args:\n        image: An input image as a numpy array.\n\n    Returns:\n        The image with bounding boxes and labels drawn on it.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.predict","title":"<code>predict(images)</code>","text":"<p>Perform object detection on one or multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>Any</code> <p>A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,     or a list of image file paths.</p> required <p>Returns:</p> Type Description <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>A list of DetectionResult for a single image input,</p> <code>list[DetectionResult] | list[list[DetectionResult]]</code> <p>or a list of lists of DetectionResult for multiple images.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def predict(self, images: Any) -&gt; list[DetectionResult] | list[list[DetectionResult]]:\n    \"\"\"\n    Perform object detection on one or multiple images.\n\n    Args:\n        images: A single image as a numpy array, a single image path as a string, a list of images as numpy arrays,\n                or a list of image file paths.\n\n    Returns:\n        A list of DetectionResult for a single image input,\n        or a list of lists of DetectionResult for multiple images.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.base.ObjectDetector.show_benchmark","title":"<code>show_benchmark(num_runs=10)</code>","text":"<p>Display the benchmark results of the model with a single random image.</p> <p>Parameters:</p> Name Type Description Default <code>num_runs</code> <code>int</code> <p>Number of times to run inference on the image for averaging.</p> <code>10</code> Displays <p>Model information and benchmark results in a formatted table.</p> Source code in <code>open_image_models/detection/core/base.py</code> <pre><code>def show_benchmark(self, num_runs: int = 10) -&gt; None:\n    \"\"\"\n    Display the benchmark results of the model with a single random image.\n\n    Args:\n        num_runs: Number of times to run inference on the image for averaging.\n\n    Displays:\n        Model information and benchmark results in a formatted table.\n    \"\"\"\n</code></pre>"},{"location":"reference/#open_image_models.detection.core.hub.PlateDetectorModel","title":"<code>PlateDetectorModel = Literal['yolo-v9-s-608-license-plate-end2end', 'yolo-v9-t-640-license-plate-end2end', 'yolo-v9-t-512-license-plate-end2end', 'yolo-v9-t-416-license-plate-end2end', 'yolo-v9-t-384-license-plate-end2end', 'yolo-v9-t-256-license-plate-end2end']</code>  <code>module-attribute</code>","text":"<p>Available ONNX models for doing detection.</p>"}]}